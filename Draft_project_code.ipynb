{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Draft project code.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1egr-NnVyMyGoOkLs-Amqr14Y9h0oD6-I",
      "authorship_tag": "ABX9TyO0kdne/IN8o8qUJUOfq+mu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smitmirani10/Sem3FinalProject/blob/main/Draft_project_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BiFaYILndWBa",
        "outputId": "40d2a31f-106b-48fa-f36b-a5816e666d15"
      },
      "source": [
        "!pip install torch\n",
        "!pip install transformers\n",
        "!pip install graphviz\n",
        "!sudo apt-get install python-dev graphviz libgraphviz-dev pkg-config\n",
        "!sudo pip install pygraphviz"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.10.0.2)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.12.5-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 40.8 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 57.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 421 kB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 74.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.2.1 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.12.5\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (0.10.1)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "pkg-config is already the newest version (0.29.1-0ubuntu2).\n",
            "python-dev is already the newest version (2.7.15~rc1-1).\n",
            "graphviz is already the newest version (2.40.1-2).\n",
            "The following additional packages will be installed:\n",
            "  libgail-common libgail18 libgtk2.0-0 libgtk2.0-bin libgtk2.0-common\n",
            "  libgvc6-plugins-gtk libxdot4\n",
            "Suggested packages:\n",
            "  gvfs\n",
            "The following NEW packages will be installed:\n",
            "  libgail-common libgail18 libgraphviz-dev libgtk2.0-0 libgtk2.0-bin\n",
            "  libgtk2.0-common libgvc6-plugins-gtk libxdot4\n",
            "0 upgraded, 8 newly installed, 0 to remove and 37 not upgraded.\n",
            "Need to get 2,120 kB of archives.\n",
            "After this operation, 7,128 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 libgtk2.0-common all 2.24.32-1ubuntu1 [125 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 libgtk2.0-0 amd64 2.24.32-1ubuntu1 [1,769 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 libgail18 amd64 2.24.32-1ubuntu1 [14.2 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/main amd64 libgail-common amd64 2.24.32-1ubuntu1 [112 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libxdot4 amd64 2.40.1-2 [15.7 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libgvc6-plugins-gtk amd64 2.40.1-2 [18.2 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libgraphviz-dev amd64 2.40.1-2 [57.3 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 libgtk2.0-bin amd64 2.24.32-1ubuntu1 [7,536 B]\n",
            "Fetched 2,120 kB in 1s (1,991 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 8.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libgtk2.0-common.\n",
            "(Reading database ... 155222 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libgtk2.0-common_2.24.32-1ubuntu1_all.deb ...\n",
            "Unpacking libgtk2.0-common (2.24.32-1ubuntu1) ...\n",
            "Selecting previously unselected package libgtk2.0-0:amd64.\n",
            "Preparing to unpack .../1-libgtk2.0-0_2.24.32-1ubuntu1_amd64.deb ...\n",
            "Unpacking libgtk2.0-0:amd64 (2.24.32-1ubuntu1) ...\n",
            "Selecting previously unselected package libgail18:amd64.\n",
            "Preparing to unpack .../2-libgail18_2.24.32-1ubuntu1_amd64.deb ...\n",
            "Unpacking libgail18:amd64 (2.24.32-1ubuntu1) ...\n",
            "Selecting previously unselected package libgail-common:amd64.\n",
            "Preparing to unpack .../3-libgail-common_2.24.32-1ubuntu1_amd64.deb ...\n",
            "Unpacking libgail-common:amd64 (2.24.32-1ubuntu1) ...\n",
            "Selecting previously unselected package libxdot4.\n",
            "Preparing to unpack .../4-libxdot4_2.40.1-2_amd64.deb ...\n",
            "Unpacking libxdot4 (2.40.1-2) ...\n",
            "Selecting previously unselected package libgvc6-plugins-gtk.\n",
            "Preparing to unpack .../5-libgvc6-plugins-gtk_2.40.1-2_amd64.deb ...\n",
            "Unpacking libgvc6-plugins-gtk (2.40.1-2) ...\n",
            "Selecting previously unselected package libgraphviz-dev.\n",
            "Preparing to unpack .../6-libgraphviz-dev_2.40.1-2_amd64.deb ...\n",
            "Unpacking libgraphviz-dev (2.40.1-2) ...\n",
            "Selecting previously unselected package libgtk2.0-bin.\n",
            "Preparing to unpack .../7-libgtk2.0-bin_2.24.32-1ubuntu1_amd64.deb ...\n",
            "Unpacking libgtk2.0-bin (2.24.32-1ubuntu1) ...\n",
            "Setting up libgtk2.0-common (2.24.32-1ubuntu1) ...\n",
            "Setting up libxdot4 (2.40.1-2) ...\n",
            "Setting up libgtk2.0-0:amd64 (2.24.32-1ubuntu1) ...\n",
            "Setting up libgail18:amd64 (2.24.32-1ubuntu1) ...\n",
            "Setting up libgail-common:amd64 (2.24.32-1ubuntu1) ...\n",
            "Setting up libgvc6-plugins-gtk (2.40.1-2) ...\n",
            "Setting up libgraphviz-dev (2.40.1-2) ...\n",
            "Setting up libgtk2.0-bin (2.24.32-1ubuntu1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Collecting pygraphviz\n",
            "  Downloading pygraphviz-1.7.zip (118 kB)\n",
            "\u001b[K     |████████████████████████████████| 118 kB 5.1 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pygraphviz\n",
            "  Building wheel for pygraphviz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pygraphviz: filename=pygraphviz-1.7-cp37-cp37m-linux_x86_64.whl size=165734 sha256=383747b4ad99e432231b93f05df40bbf46566133ec333df9e0a6b79c476c02ee\n",
            "  Stored in directory: /root/.cache/pip/wheels/8c/bc/0c/ac35392b72556e75107ff610cb31b313e8471918a6d280e34c\n",
            "Successfully built pygraphviz\n",
            "Installing collected packages: pygraphviz\n",
            "Successfully installed pygraphviz-1.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LW2hSXrmdg-V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32cb2803-4ff6-4a5e-b3d5-6bef10b8773a"
      },
      "source": [
        "from transformers import AutoModel,AutoTokenizer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import re\n",
        "import networkx as nx\n",
        "import os\n",
        "from networkx.drawing.nx_agraph import graphviz_layout, to_agraph\n",
        "import pygraphviz as pgv\n",
        "from sklearn.metrics import confusion_matrix,f1_score,accuracy_score\n",
        "%pylab inline\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore', message='foo bar')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dddxsPU0L_UE"
      },
      "source": [
        "PubMedBERT_fulltext = 'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext'\n",
        "BERT_base_uncased = 'bert-base-uncased'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOK2hsrkFOq2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acee1573-f162-4b03-ee3b-2da3da4fe219"
      },
      "source": [
        "path= '/content/drive/MyDrive/CoNLL files/'\n",
        "accuracy_list=[]\n",
        "f1_score_list=[]\n",
        "threshold_list=[]\n",
        "threshold=0.99\n",
        "\n",
        "\n",
        "#Transfer learning step\n",
        "# Loading the pre-trained BERT model\n",
        "###################################\n",
        "# Embeddings will be derived from\n",
        "# the outputs of this model\n",
        "model = AutoModel.from_pretrained(PubMedBERT_fulltext,\n",
        "                                  output_hidden_states = True,\n",
        "                                  )\n",
        "# Setting up the tokenizer\n",
        "###################################\n",
        "# This is the same tokenizer that\n",
        "# was used in the model to generate \n",
        "# embeddings to ensure consistency\n",
        "tokenizer = AutoTokenizer.from_pretrained(PubMedBERT_fulltext)\n",
        "\n",
        "def bert_text_preparation(text,tokenizer):\n",
        "    #Ref. : https://towardsdatascience.com/3-types-of-contextualized-word-embeddings-from-bert-using-transfer-learning-81fcefe3fe6d\n",
        "    marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
        "    tokenized_text = tokenizer.tokenize(marked_text)\n",
        "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "    segments_ids = [1]*len(indexed_tokens)\n",
        "    # Convert inputs to PyTorch tensors\n",
        "    tokens_tensor = torch.tensor([indexed_tokens])\n",
        "    segments_tensors = torch.tensor([segments_ids])\n",
        "    return tokenized_text, tokens_tensor, segments_tensors\n",
        "\n",
        "def get_bert_embeddings(tokens_tensor, segments_tensors, model):\n",
        "    #Ref. : https://towardsdatascience.com/3-types-of-contextualized-word-embeddings-from-bert-using-transfer-learning-81fcefe3fe6d  \n",
        "    with torch.no_grad():\n",
        "        outputs = model(tokens_tensor, segments_tensors)\n",
        "        # Removing the first hidden state\n",
        "        # The first state is the input state\n",
        "        hidden_states = outputs[2][1:]\n",
        "    # Getting embeddings from the final BERT layer\n",
        "    token_embeddings = hidden_states[-1]\n",
        "    # Collapsing the tensor into 1-dimension\n",
        "    token_embeddings = torch.squeeze(token_embeddings, dim=0)\n",
        "    # Converting torchtensors to lists\n",
        "    list_token_embeddings = [token_embed.tolist() for token_embed in token_embeddings]\n",
        "    return list_token_embeddings\n",
        "\n",
        "#method to read the next occurance of the coref no. for eg. 3\n",
        "#if it finds that number again, it will add \n",
        "#all the words that have occured from start to end to a list\n",
        "#then pass that list as a return parameter\n",
        "def add_to_dict(i,number,my_coref_list,my_word_list,df):\n",
        "  #print('number is',number,'location in df is',i)\n",
        "  temp_coref = df['coref'][i+1]\n",
        "  my_coref_list.append(df['embedding_per_token'][i+1])\n",
        "  my_word_list.append(df['words'][i+1])\n",
        "  i=i+1\n",
        "  while number not in temp_coref:\n",
        "    my_coref_list.append(df['embedding_per_token'][i+1])\n",
        "    my_word_list.append(df['words'][i+1])\n",
        "    temp_coref = df['coref'][i+1]\n",
        "    i = i+1\n",
        "  return my_coref_list,my_word_list\n",
        "\n",
        "\n",
        "#This method extracts the number from the alphanumeric mentions\n",
        "def extract_number(number):\n",
        "  #print('I am extracting letters from',number)\n",
        "  alpha_num = number\n",
        "  res = [i for i in number if i.isdigit()]\n",
        "  if (len(res)>=2):\n",
        "    number= ''.join(res)\n",
        "    #print('final number',number)\n",
        "  return number\n",
        "\n",
        "file_count=0\n",
        "for filename in os.listdir(path): \n",
        "  with open(path+filename, mode='r') as in_file,\\\n",
        "  open('/content/new.conll', mode='w') as out_file:\n",
        "    out_file.truncate(0)\n",
        "    file_count+=1\n",
        "    print('Processing file number',file_count,filename)\n",
        "    x = in_file.readlines()\n",
        "    for i in x:\n",
        "      if i!='\\n':\n",
        "        out_file.write(i)\n",
        "    in_file.close()\n",
        "    out_file.close()\n",
        "\n",
        "  in_file = open('/content/new.conll', 'r')\n",
        "  #out_file=open('/content/drive/MyDrive/CoNLL files/new.conll', mode='w')\n",
        "  count = 0\n",
        "  sentence_list =[]\n",
        "  coref_list=[]\n",
        "  token_list=[]\n",
        "  embedding_list=[]\n",
        "  column_names = [\"words\", \"coref\",\"tokens_per_word\",\"embedding_per_token\"]\n",
        "  df = pd.DataFrame(columns = column_names)\n",
        "  while True:\n",
        "      count += 1\n",
        "      # Get next line from file\n",
        "      line = in_file.readline()\n",
        "      if (count == 1):\n",
        "        line = in_file.readline()\n",
        "      #print(line)\n",
        "      #zero out the tab count before starting to read every line\n",
        "      tab_count=0\n",
        "      # if line is empty\n",
        "      # end of file is reached\n",
        "      if not line.strip():\n",
        "          break\n",
        "      list_of_words = line.split()\n",
        "      #print(list_of_words)\n",
        "      sentence_list.append(list_of_words[3])\n",
        "      coref_list.append(list_of_words[12])\n",
        "      token_list.append(None)\n",
        "      embedding_list.append(None)\n",
        "  in_file.close()\n",
        "  df['words'] = sentence_list\n",
        "  df['coref'] = coref_list\n",
        "  df['tokens_per_word'] = token_list\n",
        "  #df['tokens_per_word'] = ''\n",
        "  #df['tokens_per_word'] = df['tokens_per_word'].apply(list)\n",
        "\n",
        "  #df['embedding_per_token'] = ''\n",
        "  #df['embedding_per_token'] = df['embedding_per_token'].apply(list)\n",
        "  df['embedding_per_token'] = embedding_list\n",
        "\n",
        "  texts=[]\n",
        "  temp=[]\n",
        "  for i in range(df['words'].size):\n",
        "    #print(i)\n",
        "    if(df['words'][i]!='.'):\n",
        "      temp.append(df['words'][i])\n",
        "    if (df['words'][i]=='.'):\n",
        "      temp.append('.')\n",
        "      final_sentence = \" \".join(temp)\n",
        "      texts.append(final_sentence)\n",
        "      temp=[] \n",
        "\n",
        "  pattern = r'#'\n",
        "  tokens=[]\n",
        "  for text in texts :\n",
        "    a=text.split()\n",
        "    for i in a:\n",
        "      temp=[]\n",
        "      tokenized_text, tokens_tensor, segments_tensors = bert_text_preparation(i, tokenizer)\n",
        "      temp_tokenized_text = tokenized_text.copy()\n",
        "      temp_tokenized_text.remove('[CLS]')\n",
        "      temp_tokenized_text.remove('[SEP]')\n",
        "      for j in temp_tokenized_text:\n",
        "        temp.append(j)\n",
        "      tokens.append(temp)\n",
        "\n",
        "  for i in range(df['tokens_per_word'].size):\n",
        "    df['tokens_per_word'][i]=tokens[i]\n",
        "\n",
        "  large_tokenized_text=[]\n",
        "  large_embeddings=[]\n",
        "  for text in texts:\n",
        "    #print(text)\n",
        "    tokenized_text, tokens_tensor, segments_tensors = bert_text_preparation(text, tokenizer)\n",
        "    list_token_embeddings = get_bert_embeddings(tokens_tensor, segments_tensors, model)\n",
        "    #print(tokenized_text) \n",
        "    for i in tokenized_text:                                           \n",
        "      large_tokenized_text.append(i)\n",
        "    for j in list_token_embeddings:\n",
        "      large_embeddings.append(j)\n",
        "      \n",
        "  for i in range(df['tokens_per_word'].size):\n",
        "    temp_context_vectors=[]\n",
        "    for j in df['tokens_per_word'][i]:\n",
        "      #print(j)\n",
        "      word_index = large_tokenized_text.index(j)\n",
        "      word_embedding = large_embeddings[word_index]\n",
        "      temp_context_vectors.append(word_embedding)\n",
        "    mean_vector=np.mean(temp_context_vectors,axis=0)\n",
        "    df['embedding_per_token'][i]=mean_vector\n",
        "\n",
        "  #Main purpose is to have a dictionary and a final dataframe created for all the mentions\n",
        "  #If we have discontinuous mentions, then they will be all added together as one \n",
        "  #Their context vectors and their tokens will be combined together in a single list\n",
        "\n",
        "  i=0\n",
        "  #coref_groups = {}\n",
        "  word_groups={}\n",
        "  mentions=[]\n",
        "  counter=0\n",
        "  alpha_num=''\n",
        "  #mean_vector=[]\n",
        "  column_names = [\"index\", \"mention_no\",\"context_vectors\",\"tokens\"]\n",
        "  inter_df = pd.DataFrame(columns = column_names)\n",
        "  for i in range(df['coref'].size):\n",
        "    temp = df['coref'][i]\n",
        "    mentions = temp.split('|')\n",
        "\n",
        "    #print(mentions)\n",
        "    for mention in mentions:\n",
        "      #This is the case 1 where the mention\n",
        "      #only has a opening and no closing and longer span\n",
        "      if '(' in mention and ')' not in mention:\n",
        "        my_coref_list=[]\n",
        "        my_word_list=[]\n",
        "        x=mention.split('(')\n",
        "        number=x[1]\n",
        "        #if (re.search('[a-zA-Z]', number)):\n",
        "        #  number=extract_number(number)\n",
        "        #print('Case 1',number)\n",
        "        my_coref_list.append(df['embedding_per_token'][i])\n",
        "        my_word_list.append(df['words'][i])\n",
        "        #we call the above method which reads all the mentions\n",
        "        #until it finds the end of the mention previously encountered\n",
        "        my_coref_list,my_word_list = add_to_dict(i,number,my_coref_list,my_word_list,df)\n",
        "\n",
        "      elif '(' in mention and ')' in mention:\n",
        "        #This is the singleton case\n",
        "        #Where the mention has both opeining and closing\n",
        "        my_coref_list=[]\n",
        "        my_word_list=[]\n",
        "        x = mention.split('(')\n",
        "        number=x[1].split(')')[0]\n",
        "        #print('Case 2',number)\n",
        "        my_coref_list.append(df['embedding_per_token'][i])\n",
        "        my_word_list.append(df['words'][i])\n",
        "\n",
        "      else:\n",
        "        #print('Case 4 not here')\n",
        "        continue\n",
        "      inter_df.loc[counter]=[i,number,my_coref_list,my_word_list]\n",
        "      #coref_groups[number] = my_coref_list\n",
        "      word_groups[number] = my_word_list\n",
        "      counter+=1\n",
        "\n",
        "\n",
        "  #The purpose of this cell is to :-\n",
        "  #1. Merge all the cntext vectors and all the tokens for the discontinuous spans.\n",
        "  #2. Fid the mean of all the context vectors.\n",
        "  #3. Normalize the meaned context vectors obtained in the previous step.\n",
        "  column_names = [\"index\", \"mention_no\",\"mean_normalized_vector\",\"tokens\"]\n",
        "  final_df = pd.DataFrame(columns = column_names)\n",
        "  counter=0\n",
        "  for index, row in inter_df.iterrows():\n",
        "    x=row['mention_no']\n",
        "    if (re.search('[a-zA-Z]',x)):\n",
        "      temp_coref_list=[]\n",
        "      temp_word_list=[]\n",
        "      current_number=extract_number(x)\n",
        "      if (current_number not in final_df.values):\n",
        "        for i in range(inter_df['mention_no'].size):\n",
        "          temp = inter_df['mention_no'][i]\n",
        "          if (re.search('[a-zA-Z]',temp)) and (extract_number(temp)) == current_number:\n",
        "            #print('Adding only',inter_df['mention_no'][i],'to the list')\n",
        "            temp_coref_list+=inter_df['context_vectors'][i]\n",
        "            temp_word_list+=inter_df['tokens'][i]\n",
        "        mean_vector=np.mean(temp_coref_list,axis=0)\n",
        "        norm_mean_vector = mean_vector / np.linalg.norm(mean_vector)\n",
        "        final_df.loc[counter]=[index,current_number,norm_mean_vector,temp_word_list]\n",
        "        counter+=1\n",
        "    else:\n",
        "      mean_vector=np.mean(row['context_vectors'],axis=0)\n",
        "      norm_mean_vector = mean_vector / np.linalg.norm(mean_vector)\n",
        "      final_df.loc[counter]=[row['index'],row['mention_no'],norm_mean_vector,row['tokens']]\n",
        "      counter+=1\n",
        "\n",
        "  #Preparing the gold matrix, this is were we group all te mentions together according to the original file.\n",
        "  #n=len(final_df.index)\n",
        "  #gold_matrix=np.zeros(shape=(n,n))\n",
        "  #values = final_df['mention_no'].unique()\n",
        "  #for i in values:\n",
        "  #  x = final_df.index[final_df['mention_no'].str.contains(i, case=False)]\n",
        "    #for j in x:\n",
        "      #for k in x:\n",
        "      #  gold_matrix[j][k]=1\n",
        " # print(gold_matrix)\n",
        "  \n",
        "  final_df['temp']=final_df['mention_no']\n",
        "  n=len(final_df.index)\n",
        "  gold_matrix=np.zeros(shape=(n,n))\n",
        "  for i in range(n):\n",
        "    for j in range (n):\n",
        "      if (final_df['mention_no'][i] == final_df['temp'][j]):\n",
        "        gold_matrix[i][j]+=1\n",
        "  #print(gold_matrix)\n",
        "\n",
        "  stacked = np.row_stack(final_df['mean_normalized_vector'])\n",
        "  all_pairs_cosine_sim = np.dot(stacked,stacked.T)\n",
        "  #print(all_pairs_cosine_sim)\n",
        "  all_pairs_cosine_sim[all_pairs_cosine_sim < threshold] = 0\n",
        "  all_pairs_cosine_sim[all_pairs_cosine_sim > threshold] = 1\n",
        "\n",
        "\n",
        "  A=all_pairs_cosine_sim.tolist()\n",
        "  A= pd.DataFrame.from_records(A)\n",
        "  A=A.to_numpy().flatten()\n",
        "  B=gold_matrix.tolist()\n",
        "  B = pd.DataFrame.from_records(B)\n",
        "  B=B.to_numpy().flatten()\n",
        "\n",
        "  x=(accuracy_score(B,A)*100)\n",
        "  y=(f1_score(B,A)*100)\n",
        "\n",
        "  accuracy_list.append(x)\n",
        "  f1_score_list.append(y)\n",
        "  threshold_list.append(threshold)\n",
        "\n",
        "  print('Stats for file no.',file_count,'are :-')\n",
        "  print('Confusion matrix is :\\n',confusion_matrix(B,A))\n",
        "  print('Accuracy is :',x,'%')\n",
        "  print('F1 score is :',y,'%')"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing file number 1 11597317.conll\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:248: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stats for file no. 1 are :-\n",
            "Confusion matrix is :\n",
            " [[118258    398]\n",
            " [  1268   5392]]\n",
            "Accuracy is : 98.67056082224137 %\n",
            "F1 score is : 86.61847389558233 %\n",
            "Processing file number 2 temp.conll\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:248: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stats for file no. 2 are :-\n",
            "Confusion matrix is :\n",
            " [[10  2]\n",
            " [ 0  4]]\n",
            "Accuracy is : 87.5 %\n",
            "F1 score is : 80.0 %\n",
            "Processing file number 3 11532192.conll\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:248: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stats for file no. 3 are :-\n",
            "Confusion matrix is :\n",
            " [[1245804    1254]\n",
            " [   5666   17405]]\n",
            "Accuracy is : 99.4551734508857 %\n",
            "F1 score is : 83.41720584711238 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7NOE_25oPnTe",
        "outputId": "9fc292ab-6886-4112-d13c-2ba4305066a6"
      },
      "source": [
        "threshold=0.99\n",
        "\n",
        "all_pairs_cosine_sim[all_pairs_cosine_sim < threshold] = 0\n",
        "all_pairs_cosine_sim[all_pairs_cosine_sim > threshold] = 1\n",
        "\n",
        "\n",
        "A=all_pairs_cosine_sim.tolist()\n",
        "A= pd.DataFrame.from_records(A)\n",
        "A=A.to_numpy().flatten()\n",
        "B=gold_matrix.tolist()\n",
        "B = pd.DataFrame.from_records(B)\n",
        "B=B.to_numpy().flatten()\n",
        "\n",
        "x=(accuracy_score(B,A)*100)\n",
        "y=(f1_score(B,A)*100)\n",
        "\n",
        "accuracy_list.append(x)\n",
        "f1_score_list.append(y)\n",
        "threshold_list.append(threshold)\n",
        "\n",
        "print('Stats for file no.',file_count,'are :-')\n",
        "print('Confusion matrix is :\\n',confusion_matrix(B,A))\n",
        "print('Accuracy is :',x,'%')\n",
        "print('F1 score is :',y,'%')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stats for file no. 5 are :-\n",
            "Confusion matrix is :\n",
            " [[8 4]\n",
            " [0 4]]\n",
            "Accuracy is : 75.0 %\n",
            "F1 score is : 66.66666666666666 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OyKOiAV5OgN0",
        "outputId": "90c80ca6-3b83-4dae-c6cb-234bf4abada6"
      },
      "source": [
        "np.mean(f1_score_list,axis=0)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "83.34522658089823"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 708
        },
        "id": "Dlg83IQAFUaj",
        "outputId": "c785cbeb-8822-4243-82ae-20210b51a0c0"
      },
      "source": [
        "#reading a CoNLL file into a dataframe\n",
        "#DF has 4 columns, 2 from the conll file and rest from the tokens and embeddings that we will generate\n",
        "#1st 2 columns are the actual word and the coreference\n",
        "#3rd & 4th words are the tokens generated from that word & their corresponding embeddings generated by BERT\n",
        "in_file = open('/content/new.conll', 'r')\n",
        "#out_file=open('/content/drive/MyDrive/CoNLL files/new.conll', mode='w')\n",
        "count = 0\n",
        "sentence_list =[]\n",
        "coref_list=[]\n",
        "token_list=[]\n",
        "embedding_list=[]\n",
        "column_names = [\"words\", \"coref\",\"tokens_per_word\",\"embedding_per_token\"]\n",
        "df = pd.DataFrame(columns = column_names)\n",
        "while True:\n",
        "    count += 1\n",
        "    # Get next line from file\n",
        "    line = in_file.readline()\n",
        "    if (count == 1):\n",
        "      line = in_file.readline()\n",
        "    #print(line)\n",
        "    #zero out the tab count before starting to read every line\n",
        "    tab_count=0\n",
        "    # if line is empty\n",
        "    # end of file is reached\n",
        "    if not line.strip():\n",
        "        break\n",
        "    list_of_words = line.split()\n",
        "    #print(list_of_words)\n",
        "    sentence_list.append(list_of_words[3])\n",
        "    coref_list.append(list_of_words[12])\n",
        "    token_list.append(None)\n",
        "    embedding_list.append(None)\n",
        "in_file.close()\n",
        "df['words'] = sentence_list\n",
        "df['coref'] = coref_list\n",
        "df['tokens_per_word'] = token_list\n",
        "#df['tokens_per_word'] = ''\n",
        "#df['tokens_per_word'] = df['tokens_per_word'].apply(list)\n",
        "\n",
        "#df['embedding_per_token'] = ''\n",
        "#df['embedding_per_token'] = df['embedding_per_token'].apply(list)\n",
        "df['embedding_per_token'] = embedding_list\n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>words</th>\n",
              "      <th>coref</th>\n",
              "      <th>tokens_per_word</th>\n",
              "      <th>embedding_per_token</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Two</td>\n",
              "      <td>(32</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>recent</td>\n",
              "      <td>-</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>papers</td>\n",
              "      <td>32</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>provide</td>\n",
              "      <td>-</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>new</td>\n",
              "      <td>-</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>evidence</td>\n",
              "      <td>-</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>relevant</td>\n",
              "      <td>-</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>to</td>\n",
              "      <td>-</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>the</td>\n",
              "      <td>-</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>role</td>\n",
              "      <td>-</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>of</td>\n",
              "      <td>-</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>the</td>\n",
              "      <td>(1</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>breast</td>\n",
              "      <td>(4</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>cancer</td>\n",
              "      <td>-</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>susceptibility</td>\n",
              "      <td>-</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>gene</td>\n",
              "      <td>4)</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>BRCA2</td>\n",
              "      <td>1)</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>in</td>\n",
              "      <td>-</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>DNA</td>\n",
              "      <td>(6|(32)</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>repair</td>\n",
              "      <td>6)</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>.</td>\n",
              "      <td>-</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             words    coref tokens_per_word embedding_per_token\n",
              "0              Two      (32            None                None\n",
              "1           recent        -            None                None\n",
              "2           papers       32            None                None\n",
              "3          provide        -            None                None\n",
              "4              new        -            None                None\n",
              "5         evidence        -            None                None\n",
              "6         relevant        -            None                None\n",
              "7               to        -            None                None\n",
              "8              the        -            None                None\n",
              "9             role        -            None                None\n",
              "10              of        -            None                None\n",
              "11             the       (1            None                None\n",
              "12          breast       (4            None                None\n",
              "13          cancer        -            None                None\n",
              "14  susceptibility        -            None                None\n",
              "15            gene       4)            None                None\n",
              "16           BRCA2       1)            None                None\n",
              "17              in        -            None                None\n",
              "18             DNA  (6|(32)            None                None\n",
              "19          repair       6)            None                None\n",
              "20               .        -            None                None"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "668qlptmF0Xt"
      },
      "source": [
        "PubMedBERT_fulltext = 'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext'\n",
        "BERT_base_uncased = 'bert-base-uncased'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dU0kvo_Xdhdp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06e71683-dcf1-4cc5-c387-38177948b2ca"
      },
      "source": [
        "#Transfer learning step\n",
        "# Loading the pre-trained BERT model\n",
        "###################################\n",
        "# Embeddings will be derived from\n",
        "# the outputs of this model\n",
        "model = AutoModel.from_pretrained(PubMedBERT_fulltext,\n",
        "                                  output_hidden_states = True,\n",
        "                                  )\n",
        "\n",
        "# Setting up the tokenizer\n",
        "###################################\n",
        "# This is the same tokenizer that\n",
        "# was used in the model to generate \n",
        "# embeddings to ensure consistency\n",
        "tokenizer = AutoTokenizer.from_pretrained(PubMedBERT_fulltext)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bgz_DxwSEUyM"
      },
      "source": [
        "def bert_text_preparation(text, tokenizer):\n",
        "    #Ref. : https://towardsdatascience.com/3-types-of-contextualized-word-embeddings-from-bert-using-transfer-learning-81fcefe3fe6d\n",
        "    marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
        "    tokenized_text = tokenizer.tokenize(marked_text)\n",
        "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "    segments_ids = [1]*len(indexed_tokens)\n",
        "    # Convert inputs to PyTorch tensors\n",
        "    tokens_tensor = torch.tensor([indexed_tokens])\n",
        "    segments_tensors = torch.tensor([segments_ids])\n",
        "    return tokenized_text, tokens_tensor, segments_tensors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40wuSwe7E9-s"
      },
      "source": [
        "def get_bert_embeddings(tokens_tensor, segments_tensors, model):\n",
        "#Ref. : https://towardsdatascience.com/3-types-of-contextualized-word-embeddings-from-bert-using-transfer-learning-81fcefe3fe6d  \n",
        "    with torch.no_grad():\n",
        "        outputs = model(tokens_tensor, segments_tensors)\n",
        "        # Removing the first hidden state\n",
        "        # The first state is the input state\n",
        "        hidden_states = outputs[2][1:]\n",
        "    # Getting embeddings from the final BERT layer\n",
        "    token_embeddings = hidden_states[-1]\n",
        "    # Collapsing the tensor into 1-dimension\n",
        "    token_embeddings = torch.squeeze(token_embeddings, dim=0)\n",
        "    # Converting torchtensors to lists\n",
        "    list_token_embeddings = [token_embed.tolist() for token_embed in token_embeddings]\n",
        "    return list_token_embeddings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvWthS2iFY9v"
      },
      "source": [
        "texts=[]\n",
        "temp=[]\n",
        "for i in range(df['words'].size):\n",
        "  #print(i)\n",
        "  if(df['words'][i]!='.'):\n",
        "    temp.append(df['words'][i])\n",
        "  if (df['words'][i]=='.'):\n",
        "    temp.append('.')\n",
        "    final_sentence = \" \".join(temp)\n",
        "    texts.append(final_sentence)\n",
        "    temp=[]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9M1A7RkQIKQd"
      },
      "source": [
        "pattern = r'#'\n",
        "tokens=[]\n",
        "for text in texts :\n",
        "#if (1==1):\n",
        "  a=text.split()\n",
        "  #print(a)\n",
        "  for i in a:\n",
        "    #print(i)\n",
        "    temp=[]\n",
        "    tokenized_text, tokens_tensor, segments_tensors = bert_text_preparation(i, tokenizer)\n",
        "    temp_tokenized_text = tokenized_text.copy()\n",
        "    temp_tokenized_text.remove('[CLS]')\n",
        "    temp_tokenized_text.remove('[SEP]')\n",
        "    #print(temp_tokenized_text)\n",
        "    for j in temp_tokenized_text:\n",
        "      #print(j)\n",
        "      temp.append(j)\n",
        "    tokens.append(temp)\n",
        "#print(tokens)\n",
        "#len(tokens)\n",
        "#df['words'].size\n",
        "  #print(tokens)\n",
        "for i in range(df['coref'].size):\n",
        "  df['tokens_per_word'][i]=tokens[i]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JrMr3dMU456"
      },
      "source": [
        "large_tokenized_text=[]\n",
        "large_embeddings=[]\n",
        "for text in texts:\n",
        "  #print(text)\n",
        "  tokenized_text, tokens_tensor, segments_tensors = bert_text_preparation(text, tokenizer)\n",
        "  list_token_embeddings = get_bert_embeddings(tokens_tensor, segments_tensors, model)\n",
        "  #print(tokenized_text) \n",
        "  for i in tokenized_text:                                           \n",
        "    large_tokenized_text.append(i)\n",
        "  for j in list_token_embeddings:\n",
        "    large_embeddings.append(j)\n",
        "    \n",
        "for i in range(df['tokens_per_word'].size):\n",
        "  temp_context_vectors=[]\n",
        "  for j in df['tokens_per_word'][i]:\n",
        "    #print(j)\n",
        "    word_index = large_tokenized_text.index(j)\n",
        "    word_embedding = large_embeddings[word_index]\n",
        "    temp_context_vectors.append(word_embedding)\n",
        "  mean_vector=np.mean(temp_context_vectors,axis=0)\n",
        "  df['embedding_per_token'][i]=mean_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdDv7gb1PugK"
      },
      "source": [
        "#pattern = r'#'\n",
        "#for text in texts:\n",
        "  #reading one sentence at a time and generating its tokens\n",
        "  #as well as token embeddings from above methods\n",
        " # tokenized_text, tokens_tensor, segments_tensors = bert_text_preparation(text, tokenizer)\n",
        "  #list_token_embeddings = get_bert_embeddings(tokens_tensor, segments_tensors, model)\n",
        "  #print(tokenized_text)\n",
        "\n",
        "#here we extract the tokens,except the sep and cls and punctuations\n",
        "#and then we match which token is a substring of which cell\n",
        "#of the word column in original DF\n",
        "#when we find a match, we record the index of that item in DF\n",
        "#then we update the token & embedding column with the tokens & embeddings obtained from that word\n",
        "#for i in tokenized_text:\n",
        " # print(i)\n",
        "  #if i != \"[CLS]\" and i != \"[SEP]\" and i != \".\":\n",
        "   # word_index = tokenized_text.index(i)\n",
        "    #word_embedding = list_token_embeddings[word_index]\n",
        "    #print(word_index)\n",
        "    #if (i!='(' and i!=')'):\n",
        "      #i = re.sub(pattern, '', i)\n",
        "      #print(i)\n",
        "      #x = df.index[df['words'].str.contains(i, case=False)]\n",
        "      #print(x)\n",
        "    #print(x)\n",
        "    #for j in range(len(x)):\n",
        "      #df['tokens_per_word'][x[j]].append(i)\n",
        "      #df['tokens_per_word'][x[j]]= i\n",
        "      #df['embedding_per_token'][x[j]].append(word_embedding)\n",
        "     # df['embedding_per_token'][x[j]]=word_embedding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34bV1BYcGjhx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "goBbDogfSHJg"
      },
      "source": [
        "#method to read the next occurance of the coref no. for eg. 3\n",
        "#if it finds that number again, it will add \n",
        "#all the words that have occured from start to end to a list\n",
        "#then pass that list as a return parameter\n",
        "def add_to_dict(i,number,my_coref_list,my_word_list,df):\n",
        "  #print('number is',number,'location in df is',i)\n",
        "  temp_coref = df['coref'][i+1]\n",
        "  my_coref_list.append(df['embedding_per_token'][i+1])\n",
        "  my_word_list.append(df['words'][i+1])\n",
        "  i=i+1\n",
        "  while number not in temp_coref:\n",
        "    my_coref_list.append(df['embedding_per_token'][i+1])\n",
        "    my_word_list.append(df['words'][i+1])\n",
        "    temp_coref = df['coref'][i+1]\n",
        "    i = i+1\n",
        "  return my_coref_list,my_word_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHYiBaUGLtc0"
      },
      "source": [
        "#This method extracts the number from the alphanumeric mentions\n",
        "def extract_number(number):\n",
        "  #print('I am extracting letters from',number)\n",
        "  alpha_num = number\n",
        "  res = [i for i in number if i.isdigit()]\n",
        "  if (len(res)>=2):\n",
        "    number= ''.join(res)\n",
        "    #print('final number',number)\n",
        "  return number"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWkP_gK_EJ_r",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "outputId": "e0b16dcb-24f3-4f6a-8f51-3a5cbfff64fc"
      },
      "source": [
        "#Main purpose is to have a dictionary and a final dataframe created for all the mentions\n",
        "#If we have discontinuous mentions, then they will be all added together as one \n",
        "#Their context vectors and their tokens will be combined together in a single list\n",
        "\n",
        "i=0\n",
        "#coref_groups = {}\n",
        "word_groups={}\n",
        "mentions=[]\n",
        "counter=0\n",
        "alpha_num=''\n",
        "#mean_vector=[]\n",
        "column_names = [\"index\", \"mention_no\",\"context_vectors\",\"tokens\"]\n",
        "inter_df = pd.DataFrame(columns = column_names)\n",
        "for i in range(df['coref'].size):\n",
        "  temp = df['coref'][i]\n",
        "  mentions = temp.split('|')\n",
        "\n",
        "  #print(mentions)\n",
        "  for mention in mentions:\n",
        "    #This is the case 1 where the mention\n",
        "    #only has a opening and no closing and longer span\n",
        "    if '(' in mention and ')' not in mention:\n",
        "      my_coref_list=[]\n",
        "      my_word_list=[]\n",
        "      x=mention.split('(')\n",
        "      number=x[1]\n",
        "      #if (re.search('[a-zA-Z]', number)):\n",
        "      #  number=extract_number(number)\n",
        "      #print('Case 1',number)\n",
        "      my_coref_list.append(df['embedding_per_token'][i])\n",
        "      my_word_list.append(df['words'][i])\n",
        "      #we call the above method which reads all the mentions\n",
        "      #until it finds the end of the mention previously encountered\n",
        "      my_coref_list,my_word_list = add_to_dict(i,number,my_coref_list,my_word_list,df)\n",
        "\n",
        "    elif '(' in mention and ')' in mention:\n",
        "      #This is the singleton case\n",
        "      #Where the mention has both opeining and closing\n",
        "      my_coref_list=[]\n",
        "      my_word_list=[]\n",
        "      x = mention.split('(')\n",
        "      number=x[1].split(')')[0]\n",
        "      #print('Case 2',number)\n",
        "      my_coref_list.append(df['embedding_per_token'][i])\n",
        "      my_word_list.append(df['words'][i])\n",
        "\n",
        "    else:\n",
        "      #print('Case 4 not here')\n",
        "      continue\n",
        "    inter_df.loc[counter]=[i,number,my_coref_list,my_word_list]\n",
        "    #coref_groups[number] = my_coref_list\n",
        "    word_groups[number] = my_word_list\n",
        "    counter+=1\n",
        "\n",
        "word_groups\n",
        "inter_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>mention_no</th>\n",
              "      <th>context_vectors</th>\n",
              "      <th>tokens</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>32</td>\n",
              "      <td>[[-0.2246626764535904, -0.26710209250450134, -...</td>\n",
              "      <td>[Two, recent, papers]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>11</td>\n",
              "      <td>1</td>\n",
              "      <td>[[0.10987842082977295, -0.14930360019207, 0.05...</td>\n",
              "      <td>[the, breast, cancer, susceptibility, gene, BR...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>12</td>\n",
              "      <td>4</td>\n",
              "      <td>[[0.08874734491109848, -0.07292603701353073, -...</td>\n",
              "      <td>[breast, cancer, susceptibility, gene]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>18</td>\n",
              "      <td>6</td>\n",
              "      <td>[[0.10239171236753464, -0.007715804968029261, ...</td>\n",
              "      <td>[DNA, repair]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>18</td>\n",
              "      <td>32</td>\n",
              "      <td>[[0.10239171236753464, -0.007715804968029261, ...</td>\n",
              "      <td>[DNA]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  index  ...                                             tokens\n",
              "0     0  ...                              [Two, recent, papers]\n",
              "1    11  ...  [the, breast, cancer, susceptibility, gene, BR...\n",
              "2    12  ...             [breast, cancer, susceptibility, gene]\n",
              "3    18  ...                                      [DNA, repair]\n",
              "4    18  ...                                              [DNA]\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnxkP4RnO9bb"
      },
      "source": [
        "#The purpose of this cell is to :-\n",
        "#1. Merge all the cntext vectors and all the tokens for the discontinuous spans.\n",
        "#2. Fid the mean of all the context vectors.\n",
        "#3. Normalize the meaned context vectors obtained in the previous step.\n",
        "column_names = [\"index\", \"mention_no\",\"mean_normalized_vector\",\"tokens\"]\n",
        "final_df = pd.DataFrame(columns = column_names)\n",
        "counter=0\n",
        "for index, row in inter_df.iterrows():\n",
        "  x=row['mention_no']\n",
        "  if (re.search('[a-zA-Z]',x)):\n",
        "    temp_coref_list=[]\n",
        "    temp_word_list=[]\n",
        "    current_number=extract_number(x)\n",
        "    if (current_number not in final_df.values):\n",
        "      for i in range(inter_df['mention_no'].size):\n",
        "        temp = inter_df['mention_no'][i]\n",
        "        if (re.search('[a-zA-Z]',temp)) and (extract_number(temp)) == current_number:\n",
        "          print('Adding only',inter_df['mention_no'][i],'to the list')\n",
        "          temp_coref_list+=inter_df['context_vectors'][i]\n",
        "          temp_word_list+=inter_df['tokens'][i]\n",
        "      mean_vector=np.mean(temp_coref_list,axis=0)\n",
        "      norm_mean_vector = mean_vector / np.linalg.norm(mean_vector)\n",
        "      final_df.loc[counter]=[index,current_number,norm_mean_vector,temp_word_list]\n",
        "      counter+=1\n",
        "  else:\n",
        "    mean_vector=np.mean(row['context_vectors'],axis=0)\n",
        "    norm_mean_vector = mean_vector / np.linalg.norm(mean_vector)\n",
        "    final_df.loc[counter]=[row['index'],row['mention_no'],norm_mean_vector,row['tokens']]\n",
        "    counter+=1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "sjgks2D4DUKL",
        "outputId": "5c983637-629b-403c-9aae-f1bdaae78af7"
      },
      "source": [
        "final_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>mention_no</th>\n",
              "      <th>mean_normalized_vector</th>\n",
              "      <th>tokens</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>32</td>\n",
              "      <td>[-0.012180175937407974, -0.0026078958793962024...</td>\n",
              "      <td>[Two, recent, papers]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>11</td>\n",
              "      <td>1</td>\n",
              "      <td>[-0.0029319319533624697, -0.009076301790759205...</td>\n",
              "      <td>[the, breast, cancer, susceptibility, gene, BR...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>12</td>\n",
              "      <td>4</td>\n",
              "      <td>[-0.0035265490149949505, -0.00545502063556827,...</td>\n",
              "      <td>[breast, cancer, susceptibility, gene]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>18</td>\n",
              "      <td>6</td>\n",
              "      <td>[0.009159104415653397, -0.0028121201177927005,...</td>\n",
              "      <td>[DNA, repair]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>18</td>\n",
              "      <td>32</td>\n",
              "      <td>[0.006975753121560978, -0.0005256631552159846,...</td>\n",
              "      <td>[DNA]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  index  ...                                             tokens\n",
              "0     0  ...                              [Two, recent, papers]\n",
              "1    11  ...  [the, breast, cancer, susceptibility, gene, BR...\n",
              "2    12  ...             [breast, cancer, susceptibility, gene]\n",
              "3    18  ...                                      [DNA, repair]\n",
              "4    18  ...                                              [DNA]\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7KlBOuZHHpn",
        "outputId": "674adebe-2f3f-4155-a7fa-a049f15dc4eb"
      },
      "source": [
        "#Preparing the gold matrix, this is were we group all te mentions together according to the original file.\n",
        "final_df['temp']=final_df['mention_no']\n",
        "n=len(final_df.index)\n",
        "gold_matrix=np.zeros(shape=(n,n))\n",
        "for i in range(n):\n",
        "  for j in range(n):\n",
        "    if (final_df.iloc[[i]]['mention_no'][i] == final_df.iloc[[j]]['temp'][j]):\n",
        "      gold_matrix[i][j]+=1\n",
        "      #gold_matrix[j][i]+=1\n",
        "gold_matrix"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0., 0., 1.],\n",
              "       [0., 1., 0., 0., 0.],\n",
              "       [0., 0., 1., 0., 0.],\n",
              "       [0., 0., 0., 1., 0.],\n",
              "       [1., 0., 0., 0., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "WxZxQaulskUI",
        "outputId": "83f50436-a207-4474-8217-27c0be4db3dd"
      },
      "source": [
        "final_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>mention_no</th>\n",
              "      <th>mean_normalized_vector</th>\n",
              "      <th>tokens</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>32</td>\n",
              "      <td>[-0.012180175937407974, -0.0026078958793962024...</td>\n",
              "      <td>[Two, recent, papers]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>11</td>\n",
              "      <td>1</td>\n",
              "      <td>[-0.0029319319533624697, -0.009076301790759205...</td>\n",
              "      <td>[the, breast, cancer, susceptibility, gene, BR...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>12</td>\n",
              "      <td>4</td>\n",
              "      <td>[-0.0035265490149949505, -0.00545502063556827,...</td>\n",
              "      <td>[breast, cancer, susceptibility, gene]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>18</td>\n",
              "      <td>6</td>\n",
              "      <td>[0.009159104415653397, -0.0028121201177927005,...</td>\n",
              "      <td>[DNA, repair]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>18</td>\n",
              "      <td>32</td>\n",
              "      <td>[0.006975753121560978, -0.0005256631552159846,...</td>\n",
              "      <td>[DNA]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  index  ...                                             tokens\n",
              "0     0  ...                              [Two, recent, papers]\n",
              "1    11  ...  [the, breast, cancer, susceptibility, gene, BR...\n",
              "2    12  ...             [breast, cancer, susceptibility, gene]\n",
              "3    18  ...                                      [DNA, repair]\n",
              "4    18  ...                                              [DNA]\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFOvS_dipuCa",
        "outputId": "2b63990c-00ea-4b73-d7ba-0c6e3cd6eb05"
      },
      "source": [
        "n=len(final_df.index)\n",
        "gold_matrix=np.zeros(shape=(n,n))\n",
        "values = final_df['mention_no'].unique()\n",
        "for i in values:\n",
        "  x = final_df.index[final_df['mention_no'].str.contains(i, case=False)]\n",
        "  print(x)\n",
        "  for j in x:\n",
        "    for k in x:\n",
        "      gold_matrix[j][k]=1\n",
        "gold_matrix"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Int64Index([0, 4], dtype='int64')\n",
            "Int64Index([1], dtype='int64')\n",
            "Int64Index([2], dtype='int64')\n",
            "Int64Index([3], dtype='int64')\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0., 0., 1.],\n",
              "       [0., 1., 0., 0., 0.],\n",
              "       [0., 0., 1., 0., 0.],\n",
              "       [0., 0., 0., 1., 0.],\n",
              "       [1., 0., 0., 0., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "EK2UpY5owWD8",
        "outputId": "fc8a28e2-a9f4-4d44-c3d9-c19a66e339ec"
      },
      "source": [
        "final_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>mention_no</th>\n",
              "      <th>mean_normalized_vector</th>\n",
              "      <th>tokens</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>[-0.012180175937407974, -0.0026078958793962024...</td>\n",
              "      <td>[Two, recent, papers]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>11</td>\n",
              "      <td>1</td>\n",
              "      <td>[-0.0029319319533624697, -0.009076301790759205...</td>\n",
              "      <td>[the, breast, cancer, susceptibility, gene, BR...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>12</td>\n",
              "      <td>4</td>\n",
              "      <td>[-0.0035265490149949505, -0.00545502063556827,...</td>\n",
              "      <td>[breast, cancer, susceptibility, gene]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>18</td>\n",
              "      <td>6</td>\n",
              "      <td>[0.009159104415653397, -0.0028121201177927005,...</td>\n",
              "      <td>[DNA, repair]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>18</td>\n",
              "      <td>5</td>\n",
              "      <td>[0.006975753121560978, -0.0005256631552159846,...</td>\n",
              "      <td>[DNA]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  index  ...                                             tokens\n",
              "0     0  ...                              [Two, recent, papers]\n",
              "1    11  ...  [the, breast, cancer, susceptibility, gene, BR...\n",
              "2    12  ...             [breast, cancer, susceptibility, gene]\n",
              "3    18  ...                                      [DNA, repair]\n",
              "4    18  ...                                              [DNA]\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDne712i3fA-",
        "outputId": "89a75c91-f961-4a94-c3f5-197f9605804c"
      },
      "source": [
        "stacked = np.row_stack(final_df['mean_normalized_vector'])\n",
        "all_pairs_cosine_sim = np.dot(stacked,stacked.T)\n",
        "print(all_pairs_cosine_sim)\n",
        "all_pairs_cosine_sim[all_pairs_cosine_sim < 0.96] = 0\n",
        "all_pairs_cosine_sim[all_pairs_cosine_sim > 0.96] = 1\n",
        "all_pairs_cosine_sim"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.         0.93833288 0.92468911 0.91544165 0.90833566]\n",
            " [0.93833288 1.         0.99442806 0.96135093 0.95625727]\n",
            " [0.92468911 0.99442806 1.         0.95165305 0.94642811]\n",
            " [0.91544165 0.96135093 0.95165305 1.         0.99025422]\n",
            " [0.90833566 0.95625727 0.94642811 0.99025422 1.        ]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0., 0., 0.],\n",
              "       [0., 1., 1., 1., 0.],\n",
              "       [0., 1., 1., 0., 0.],\n",
              "       [0., 1., 0., 1., 1.],\n",
              "       [0., 0., 0., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "faDhlVYRyJg9",
        "outputId": "e13620bd-bac6-42df-d873-33fd6ff2e539"
      },
      "source": [
        "A=all_pairs_cosine_sim.tolist()\n",
        "A= pd.DataFrame.from_records(A)\n",
        "A=A.to_numpy().flatten()\n",
        "B=gold_matrix.tolist()\n",
        "B = pd.DataFrame.from_records(B)\n",
        "B=B.to_numpy().flatten()\n",
        "print('Confusion matrix is :\\n',confusion_matrix(B,A))\n",
        "print('Accuracy is :',accuracy_score(B,A)*100,'%')\n",
        "print('F1 score is :', f1_score(B,A)*100,'%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion matrix is :\n",
            " [[12  6]\n",
            " [ 2  5]]\n",
            "Accuracy is : 68.0 %\n",
            "F1 score is : 55.55555555555556 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qp_FqzUQWJd0"
      },
      "source": [
        "#Gold standard actual graph. Preparing the graph for the gold standard matrix we have obtained in previous step.\n",
        "#G = nx.DiGraph()\n",
        "#for i in range (len(final_df.index)):\n",
        "  #G.add_node(i,label=final_df['tokens'][i])\n",
        "\n",
        "#for i in range (len(final_df.index)):\n",
        " # for j in range(i):\n",
        "  #  if (final_df['mention_no'][i] == final_df['mention_no'][j]):\n",
        "   #   G.add_edge(i,j,arrowsize=0.0)\n",
        "\n",
        "# set defaults\n",
        "#G.graph['graph']={'rankdir':'TD'}\n",
        "#G.graph['node']={'shape':'rectangle'}\n",
        "#G.graph['edges']={'arrowsize':'0.0'}\n",
        "\n",
        "#A = to_agraph(G)\n",
        "#print(A)\n",
        "#A.layout('circo')\n",
        "#A.draw('gold_standard.png')\n",
        "#img = mpimg.imread('/content/gold_standard.png')\n",
        "#imgplot = plt.imshow(img)\n",
        "#plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEYoNb6X7YWS"
      },
      "source": [
        "#Predicted graph\n",
        "#G = nx.DiGraph()\n",
        "#for i in range (len(all_pairs_cosine_sim)):\n",
        "  #G.add_node(i,label=final_df['tokens'][i])\n",
        "  #for j in range (i):\n",
        "    #if (i!=j) and (all_pairs_cosine_sim[i][j]>0.0):\n",
        "      #print('hello')\n",
        "     # G.add_edge(i,j,arrowsize=0.0)\n",
        "\n",
        "# set defaults\n",
        "#G.graph['graph']={'rankdir':'TD'}\n",
        "#G.graph['node']={'shape':'rectangle'}\n",
        "#G.graph['edges']={'arrowsize':'0.0'}\n",
        "\n",
        "#A = to_agraph(G)\n",
        "#print(A)\n",
        "#A.layout('circo')\n",
        "#A.draw('predicted_coreference.png')\n",
        "#img = mpimg.imread('/content/predicted_coreference.png')\n",
        "#imgplot = plt.imshow(img)\n",
        "#plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}