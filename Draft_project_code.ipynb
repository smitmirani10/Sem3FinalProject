{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Draft project code.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1egr-NnVyMyGoOkLs-Amqr14Y9h0oD6-I",
      "authorship_tag": "ABX9TyPORambNj8ONP6Xxy9M5AMv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smitmirani10/Sem3FinalProject/blob/main/Draft_project_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BiFaYILndWBa",
        "outputId": "886d155c-c6e9-4bed-e3a3-bd3f6172734f"
      },
      "source": [
        "!pip install torch\n",
        "!pip install transformers\n",
        "!pip install graphviz\n",
        "!sudo apt-get install python-dev graphviz libgraphviz-dev pkg-config\n",
        "!sudo pip install pygraphviz"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.10.0.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.12.5)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.1.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (0.10.1)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "pkg-config is already the newest version (0.29.1-0ubuntu2).\n",
            "python-dev is already the newest version (2.7.15~rc1-1).\n",
            "graphviz is already the newest version (2.40.1-2).\n",
            "libgraphviz-dev is already the newest version (2.40.1-2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 37 not upgraded.\n",
            "Requirement already satisfied: pygraphviz in /usr/local/lib/python3.7/dist-packages (1.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LW2hSXrmdg-V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c999ec90-0986-4cad-98c7-bd05d14348d8"
      },
      "source": [
        "from transformers import AutoModel,AutoTokenizer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import re\n",
        "import networkx as nx\n",
        "import pylab as plt\n",
        "from networkx.drawing.nx_agraph import graphviz_layout, to_agraph\n",
        "import pygraphviz as pgv\n",
        "from sklearn.metrics import confusion_matrix,f1_score,accuracy_score\n",
        "%pylab inline\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/magics/pylab.py:161: UserWarning: pylab import has clobbered these variables: ['text', 'plt', 'number']\n",
            "`%matplotlib` prevents importing * from pylab and numpy\n",
            "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oM-PSc8f6yhk"
      },
      "source": [
        "with open('/content/drive/MyDrive/CoNLL files/new_temp.conll', mode='r') as in_file, \\\n",
        "     open('/content/drive/MyDrive/CoNLL files/new.conll', mode='w') as out_file:\n",
        "    x = in_file.readlines()\n",
        "    for i in x:\n",
        "      if i!='\\n':\n",
        "        out_file.write(i)\n",
        "    in_file.close()\n",
        "    out_file.close()"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "Dlg83IQAFUaj",
        "outputId": "df3842a0-acba-4da9-aebf-e40d3f920166"
      },
      "source": [
        "#reading a CoNLL file into a dataframe\n",
        "#DF has 4 columns, 2 from the conll file and rest from the tokens and embeddings that we will generate\n",
        "#1st 2 columns are the actual word and the coreference\n",
        "#3rd & 4th words are the tokens generated from that word & their corresponding embeddings generated by BERT\n",
        "in_file = open('/content/drive/MyDrive/CoNLL files/new.conll', 'r')\n",
        "#out_file=open('/content/drive/MyDrive/CoNLL files/new.conll', mode='w')\n",
        "count = 0\n",
        "sentence_list =[]\n",
        "coref_list=[]\n",
        "token_list=[]\n",
        "embedding_list=[]\n",
        "column_names = [\"words\", \"coref\",\"tokens_per_word\",\"embedding_per_token\"]\n",
        "df = pd.DataFrame(columns = column_names)\n",
        "while True:\n",
        "    count += 1\n",
        "    # Get next line from file\n",
        "    line = in_file.readline()\n",
        "    if (count == 1):\n",
        "      line = in_file.readline()\n",
        "    #print(line)\n",
        "    #zero out the tab count before starting to read every line\n",
        "    tab_count=0\n",
        "    # if line is empty\n",
        "    # end of file is reached\n",
        "    if not line.strip():\n",
        "        break\n",
        "    list_of_words = line.split()\n",
        "    #print(list_of_words)\n",
        "    sentence_list.append(list_of_words[3])\n",
        "    coref_list.append(list_of_words[12])\n",
        "    token_list.append(None)\n",
        "    embedding_list.append(None)\n",
        "in_file.close()\n",
        "df['words'] = sentence_list\n",
        "df['coref'] = coref_list\n",
        "df['tokens_per_word'] = token_list\n",
        "#df['tokens_per_word'] = ''\n",
        "#df['tokens_per_word'] = df['tokens_per_word'].apply(list)\n",
        "\n",
        "#df['embedding_per_token'] = ''\n",
        "#df['embedding_per_token'] = df['embedding_per_token'].apply(list)\n",
        "df['embedding_per_token'] = embedding_list\n",
        "df"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>words</th>\n",
              "      <th>coref</th>\n",
              "      <th>tokens_per_word</th>\n",
              "      <th>embedding_per_token</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Diabetes</td>\n",
              "      <td>(1</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Insipidus</td>\n",
              "      <td>-</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>in</td>\n",
              "      <td>-</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Mice</td>\n",
              "      <td>(2</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>with</td>\n",
              "      <td>-</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>647</th>\n",
              "      <td>kidney</td>\n",
              "      <td>6)</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>648</th>\n",
              "      <td>[</td>\n",
              "      <td>-</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>649</th>\n",
              "      <td>5</td>\n",
              "      <td>-</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>650</th>\n",
              "      <td>]</td>\n",
              "      <td>-</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>651</th>\n",
              "      <td>.</td>\n",
              "      <td>-</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>652 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         words coref tokens_per_word embedding_per_token\n",
              "0     Diabetes    (1            None                None\n",
              "1    Insipidus     -            None                None\n",
              "2           in     -            None                None\n",
              "3         Mice    (2            None                None\n",
              "4         with     -            None                None\n",
              "..         ...   ...             ...                 ...\n",
              "647     kidney    6)            None                None\n",
              "648          [     -            None                None\n",
              "649          5     -            None                None\n",
              "650          ]     -            None                None\n",
              "651          .     -            None                None\n",
              "\n",
              "[652 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "668qlptmF0Xt"
      },
      "source": [
        "PubMedBERT_fulltext = 'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext'\n",
        "BERT_base_uncased = 'bert-base-uncased'\n"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dU0kvo_Xdhdp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45e512ab-bab8-41c2-e145-d700b5bdcc11"
      },
      "source": [
        "#Transfer learning step\n",
        "# Loading the pre-trained BERT model\n",
        "###################################\n",
        "# Embeddings will be derived from\n",
        "# the outputs of this model\n",
        "model = AutoModel.from_pretrained(PubMedBERT_fulltext,\n",
        "                                  output_hidden_states = True,\n",
        "                                  )\n",
        "\n",
        "# Setting up the tokenizer\n",
        "###################################\n",
        "# This is the same tokenizer that\n",
        "# was used in the model to generate \n",
        "# embeddings to ensure consistency\n",
        "tokenizer = AutoTokenizer.from_pretrained(PubMedBERT_fulltext)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bgz_DxwSEUyM"
      },
      "source": [
        "def bert_text_preparation(text, tokenizer):\n",
        "    #Ref. : https://towardsdatascience.com/3-types-of-contextualized-word-embeddings-from-bert-using-transfer-learning-81fcefe3fe6d\n",
        "    marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
        "    tokenized_text = tokenizer.tokenize(marked_text)\n",
        "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "    segments_ids = [1]*len(indexed_tokens)\n",
        "    # Convert inputs to PyTorch tensors\n",
        "    tokens_tensor = torch.tensor([indexed_tokens])\n",
        "    segments_tensors = torch.tensor([segments_ids])\n",
        "    return tokenized_text, tokens_tensor, segments_tensors"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40wuSwe7E9-s"
      },
      "source": [
        "def get_bert_embeddings(tokens_tensor, segments_tensors, model):\n",
        "#Ref. : https://towardsdatascience.com/3-types-of-contextualized-word-embeddings-from-bert-using-transfer-learning-81fcefe3fe6d  \n",
        "    with torch.no_grad():\n",
        "        outputs = model(tokens_tensor, segments_tensors)\n",
        "        # Removing the first hidden state\n",
        "        # The first state is the input state\n",
        "        hidden_states = outputs[2][1:]\n",
        "    # Getting embeddings from the final BERT layer\n",
        "    token_embeddings = hidden_states[-1]\n",
        "    # Collapsing the tensor into 1-dimension\n",
        "    token_embeddings = torch.squeeze(token_embeddings, dim=0)\n",
        "    # Converting torchtensors to lists\n",
        "    list_token_embeddings = [token_embed.tolist() for token_embed in token_embeddings]\n",
        "    return list_token_embeddings"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvWthS2iFY9v"
      },
      "source": [
        "texts=[]\n",
        "temp=[]\n",
        "for i in range(df['words'].size):\n",
        "  #print(i)\n",
        "  if(df['words'][i]!='.'):\n",
        "    temp.append(df['words'][i])\n",
        "  if (df['words'][i]=='.'):\n",
        "    temp.append('.')\n",
        "    final_sentence = \" \".join(temp)\n",
        "    texts.append(final_sentence)\n",
        "    temp=[]"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9M1A7RkQIKQd"
      },
      "source": [
        "pattern = r'#'\n",
        "tokens=[]\n",
        "for text in texts :\n",
        "#if (1==1):\n",
        "  a=text.split()\n",
        "  #print(a)\n",
        "  for i in a:\n",
        "    #print(i)\n",
        "    temp=[]\n",
        "    tokenized_text, tokens_tensor, segments_tensors = bert_text_preparation(i, tokenizer)\n",
        "    temp_tokenized_text = tokenized_text.copy()\n",
        "    temp_tokenized_text.remove('[CLS]')\n",
        "    temp_tokenized_text.remove('[SEP]')\n",
        "    #print(temp_tokenized_text)\n",
        "    for j in temp_tokenized_text:\n",
        "      #print(j)\n",
        "      temp.append(j)\n",
        "    tokens.append(temp)\n",
        "#print(tokens)\n",
        "#len(tokens)\n",
        "#df['words'].size\n",
        "  #print(tokens)\n",
        "for i in range(df['coref'].size):\n",
        "  df['tokens_per_word'][i]=tokens[i]"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JrMr3dMU456"
      },
      "source": [
        "large_tokenized_text=[]\n",
        "large_embeddings=[]\n",
        "for text in texts:\n",
        "  #print(text)\n",
        "  tokenized_text, tokens_tensor, segments_tensors = bert_text_preparation(text, tokenizer)\n",
        "  list_token_embeddings = get_bert_embeddings(tokens_tensor, segments_tensors, model)\n",
        "  #print(tokenized_text) \n",
        "  for i in tokenized_text:                                           \n",
        "    large_tokenized_text.append(i)\n",
        "  for j in list_token_embeddings:\n",
        "    large_embeddings.append(j)\n",
        "    \n",
        "for i in range(df['tokens_per_word'].size):\n",
        "  temp_context_vectors=[]\n",
        "  for j in df['tokens_per_word'][i]:\n",
        "    #print(j)\n",
        "    word_index = large_tokenized_text.index(j)\n",
        "    word_embedding = large_embeddings[word_index]\n",
        "    temp_context_vectors.append(word_embedding)\n",
        "  mean_vector=np.mean(temp_context_vectors,axis=0)\n",
        "  df['embedding_per_token'][i]=mean_vector"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdDv7gb1PugK"
      },
      "source": [
        "#pattern = r'#'\n",
        "#for text in texts:\n",
        "  #reading one sentence at a time and generating its tokens\n",
        "  #as well as token embeddings from above methods\n",
        " # tokenized_text, tokens_tensor, segments_tensors = bert_text_preparation(text, tokenizer)\n",
        "  #list_token_embeddings = get_bert_embeddings(tokens_tensor, segments_tensors, model)\n",
        "  #print(tokenized_text)\n",
        "\n",
        "#here we extract the tokens,except the sep and cls and punctuations\n",
        "#and then we match which token is a substring of which cell\n",
        "#of the word column in original DF\n",
        "#when we find a match, we record the index of that item in DF\n",
        "#then we update the token & embedding column with the tokens & embeddings obtained from that word\n",
        "#for i in tokenized_text:\n",
        " # print(i)\n",
        "  #if i != \"[CLS]\" and i != \"[SEP]\" and i != \".\":\n",
        "   # word_index = tokenized_text.index(i)\n",
        "    #word_embedding = list_token_embeddings[word_index]\n",
        "    #print(word_index)\n",
        "    #if (i!='(' and i!=')'):\n",
        "      #i = re.sub(pattern, '', i)\n",
        "      #print(i)\n",
        "      #x = df.index[df['words'].str.contains(i, case=False)]\n",
        "      #print(x)\n",
        "    #print(x)\n",
        "    #for j in range(len(x)):\n",
        "      #df['tokens_per_word'][x[j]].append(i)\n",
        "      #df['tokens_per_word'][x[j]]= i\n",
        "      #df['embedding_per_token'][x[j]].append(word_embedding)\n",
        "     # df['embedding_per_token'][x[j]]=word_embedding"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34bV1BYcGjhx"
      },
      "source": [
        ""
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "goBbDogfSHJg"
      },
      "source": [
        "#method to read the next occurance of the coref no. for eg. 3\n",
        "#if it finds that number again, it will add \n",
        "#all the words that have occured from start to end to a list\n",
        "#then pass that list as a return parameter\n",
        "def add_to_dict(i,number,my_coref_list,my_word_list,df):\n",
        "  #print('number is',number,'location in df is',i)\n",
        "  temp_coref = df['coref'][i+1]\n",
        "  my_coref_list.append(df['embedding_per_token'][i+1])\n",
        "  my_word_list.append(df['words'][i+1])\n",
        "  i=i+1\n",
        "  while number not in temp_coref:\n",
        "    my_coref_list.append(df['embedding_per_token'][i+1])\n",
        "    my_word_list.append(df['words'][i+1])\n",
        "    temp_coref = df['coref'][i+1]\n",
        "    i = i+1\n",
        "  return my_coref_list,my_word_list"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHYiBaUGLtc0"
      },
      "source": [
        "#This method extracts the number from the alphanumeric mentions\n",
        "def extract_number(number):\n",
        "  #print('I am extracting letters from',number)\n",
        "  alpha_num = number\n",
        "  res = [i for i in number if i.isdigit()]\n",
        "  if (len(res)>=2):\n",
        "    number= ''.join(res)\n",
        "    #print('final number',number)\n",
        "  return number"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWkP_gK_EJ_r",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 597
        },
        "outputId": "96042958-ba36-4407-b8b7-6e75e146834a"
      },
      "source": [
        "#Main purpose is to have a dictionary and a final dataframe created for all the mentions\n",
        "#If we have discontinuous mentions, then they will be all added together as one \n",
        "#Their context vectors and their tokens will be combined together in a single list\n",
        "\n",
        "i=0\n",
        "#coref_groups = {}\n",
        "word_groups={}\n",
        "mentions=[]\n",
        "counter=0\n",
        "alpha_num=''\n",
        "#mean_vector=[]\n",
        "column_names = [\"index\", \"mention_no\",\"context_vectors\",\"tokens\"]\n",
        "inter_df = pd.DataFrame(columns = column_names)\n",
        "for i in range(df['coref'].size):\n",
        "  temp = df['coref'][i]\n",
        "  mentions = temp.split('|')\n",
        "\n",
        "  #print(mentions)\n",
        "  for mention in mentions:\n",
        "    #This is the case 1 where the mention\n",
        "    #only has a opening and no closing and longer span\n",
        "    if '(' in mention and ')' not in mention:\n",
        "      my_coref_list=[]\n",
        "      my_word_list=[]\n",
        "      x=mention.split('(')\n",
        "      number=x[1]\n",
        "      #if (re.search('[a-zA-Z]', number)):\n",
        "      #  number=extract_number(number)\n",
        "      #print('Case 1',number)\n",
        "      my_coref_list.append(df['embedding_per_token'][i])\n",
        "      my_word_list.append(df['words'][i])\n",
        "      #we call the above method which reads all the mentions\n",
        "      #until it finds the end of the mention previously encountered\n",
        "      my_coref_list,my_word_list = add_to_dict(i,number,my_coref_list,my_word_list,df)\n",
        "\n",
        "    elif '(' in mention and ')' in mention:\n",
        "      #This is the singleton case\n",
        "      #Where the mention has both opeining and closing\n",
        "      my_coref_list=[]\n",
        "      my_word_list=[]\n",
        "      x = mention.split('(')\n",
        "      number=x[1].split(')')[0]\n",
        "      #print('Case 2',number)\n",
        "      my_coref_list.append(df['embedding_per_token'][i])\n",
        "      my_word_list.append(df['words'][i])\n",
        "\n",
        "    else:\n",
        "      #print('Case 4 not here')\n",
        "      continue\n",
        "    inter_df.loc[counter]=[i,number,my_coref_list,my_word_list]\n",
        "    #coref_groups[number] = my_coref_list\n",
        "    word_groups[number] = my_word_list\n",
        "    counter+=1\n",
        "\n",
        "word_groups\n",
        "inter_df"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>mention_no</th>\n",
              "      <th>context_vectors</th>\n",
              "      <th>tokens</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>[[-0.22877123951911926, 0.20585937798023224, 0...</td>\n",
              "      <td>[Diabetes, Insipidus, in, Mice, with, a, Mutat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>[[-0.10411273688077927, -0.04446271434426308, ...</td>\n",
              "      <td>[Mice, with, a, Mutation, in, Aquaporin, -, 2]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>[[0.1659092754125595, -0.11474323272705078, -0...</td>\n",
              "      <td>[a, Mutation, in, Aquaporin, -, 2]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>[[0.03711164277046919, -0.06261884421110153, -...</td>\n",
              "      <td>[Aquaporin, -, 2]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>12</td>\n",
              "      <td>5</td>\n",
              "      <td>[[-0.2006397396326065, -0.16753144562244415, 0...</td>\n",
              "      <td>[Congenital, nephrogenic, diabetes, insipidus]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>109</th>\n",
              "      <td>616</td>\n",
              "      <td>20</td>\n",
              "      <td>[[-0.0694167222827673, 0.010410560294985771, -...</td>\n",
              "      <td>[AQP2]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>110</th>\n",
              "      <td>629</td>\n",
              "      <td>19</td>\n",
              "      <td>[[-0.21900470554828644, 0.09637747704982758, -...</td>\n",
              "      <td>[water]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111</th>\n",
              "      <td>636</td>\n",
              "      <td>20</td>\n",
              "      <td>[[0.0586155503988266, -0.19091956317424774, -0...</td>\n",
              "      <td>[it]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>112</th>\n",
              "      <td>640</td>\n",
              "      <td>40</td>\n",
              "      <td>[[-0.012681677006185055, 0.20679669082164764, ...</td>\n",
              "      <td>[collecting, -, duct, principal, cells]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>113</th>\n",
              "      <td>646</td>\n",
              "      <td>6</td>\n",
              "      <td>[[0.07215554267168045, 0.35096317529678345, 0....</td>\n",
              "      <td>[the, kidney]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>114 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    index  ...                                             tokens\n",
              "0       0  ...  [Diabetes, Insipidus, in, Mice, with, a, Mutat...\n",
              "1       3  ...     [Mice, with, a, Mutation, in, Aquaporin, -, 2]\n",
              "2       5  ...                 [a, Mutation, in, Aquaporin, -, 2]\n",
              "3       8  ...                                  [Aquaporin, -, 2]\n",
              "4      12  ...     [Congenital, nephrogenic, diabetes, insipidus]\n",
              "..    ...  ...                                                ...\n",
              "109   616  ...                                             [AQP2]\n",
              "110   629  ...                                            [water]\n",
              "111   636  ...                                               [it]\n",
              "112   640  ...            [collecting, -, duct, principal, cells]\n",
              "113   646  ...                                      [the, kidney]\n",
              "\n",
              "[114 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnxkP4RnO9bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6100012b-1f34-4810-a571-7df03a04301f"
      },
      "source": [
        "#The purpose of this cell is to :-\n",
        "#1. Merge all the cntext vectors and all the tokens for the discontinuous spans.\n",
        "#2. Fid the mean of all the context vectors.\n",
        "#3. Normalize the meaned context vectors obtained in the previous step.\n",
        "column_names = [\"index\", \"mention_no\",\"mean_normalized_vector\",\"tokens\"]\n",
        "final_df = pd.DataFrame(columns = column_names)\n",
        "counter=0\n",
        "for index, row in inter_df.iterrows():\n",
        "  x=row['mention_no']\n",
        "  if (re.search('[a-zA-Z]',x)):\n",
        "    temp_coref_list=[]\n",
        "    temp_word_list=[]\n",
        "    current_number=extract_number(x)\n",
        "    if (current_number not in final_df.values):\n",
        "      for i in range(inter_df['mention_no'].size):\n",
        "        temp = inter_df['mention_no'][i]\n",
        "        if (re.search('[a-zA-Z]',temp)) and (extract_number(temp)) == current_number:\n",
        "          print('Adding only',inter_df['mention_no'][i],'to the list')\n",
        "          temp_coref_list+=inter_df['context_vectors'][i]\n",
        "          temp_word_list+=inter_df['tokens'][i]\n",
        "      mean_vector=np.mean(temp_coref_list,axis=0)\n",
        "      norm_mean_vector = mean_vector / np.linalg.norm(mean_vector)\n",
        "      final_df.loc[counter]=[index,current_number,norm_mean_vector,temp_word_list]\n",
        "      counter+=1\n",
        "  else:\n",
        "    mean_vector=np.mean(row['context_vectors'],axis=0)\n",
        "    norm_mean_vector = mean_vector / np.linalg.norm(mean_vector)\n",
        "    final_df.loc[counter]=[row['index'],row['mention_no'],norm_mean_vector,row['tokens']]\n",
        "    counter+=1\n"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:14: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adding only 3a to the list\n",
            "Adding only 3a to the list\n",
            "Adding only 24a to the list\n",
            "Adding only 24a to the list\n",
            "Adding only 24b to the list\n",
            "Adding only 24b to the list\n",
            "Adding only 28b to the list\n",
            "Adding only 28b to the list\n",
            "Adding only 28a to the list\n",
            "Adding only 28a to the list\n",
            "Adding only 31a to the list\n",
            "Adding only 31a to the list\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 597
        },
        "id": "sjgks2D4DUKL",
        "outputId": "5f402531-913a-4ecc-f097-dd735030f3c3"
      },
      "source": [
        "final_df"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>mention_no</th>\n",
              "      <th>mean_normalized_vector</th>\n",
              "      <th>tokens</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>[-0.002128100072283943, 0.00035211816842894246...</td>\n",
              "      <td>[Diabetes, Insipidus, in, Mice, with, a, Mutat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>[0.0025281501780507433, -0.0013584542354517226...</td>\n",
              "      <td>[Mice, with, a, Mutation, in, Aquaporin, -, 2]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>[0.0026430296095377877, -0.0012788860062432866...</td>\n",
              "      <td>[a, Mutation, in, Aquaporin, -, 2]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>[0.005380736439015781, 0.002337076059069516, -...</td>\n",
              "      <td>[Aquaporin, -, 2]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>12</td>\n",
              "      <td>5</td>\n",
              "      <td>[-0.017618984880443493, 0.0016945110431218614,...</td>\n",
              "      <td>[Congenital, nephrogenic, diabetes, insipidus]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>616</td>\n",
              "      <td>20</td>\n",
              "      <td>[-0.0047985462672887275, 0.000719647278077975,...</td>\n",
              "      <td>[AQP2]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>629</td>\n",
              "      <td>19</td>\n",
              "      <td>[-0.014801009022190803, 0.006513485195577097, ...</td>\n",
              "      <td>[water]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>636</td>\n",
              "      <td>20</td>\n",
              "      <td>[0.0039581983385687846, -0.012892440531816853,...</td>\n",
              "      <td>[it]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>640</td>\n",
              "      <td>40</td>\n",
              "      <td>[0.005998361372358761, 0.019226518188499176, -...</td>\n",
              "      <td>[collecting, -, duct, principal, cells]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>103</th>\n",
              "      <td>646</td>\n",
              "      <td>6</td>\n",
              "      <td>[0.0007760625469087869, 0.026091660306506205, ...</td>\n",
              "      <td>[the, kidney]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>104 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    index  ...                                             tokens\n",
              "0       0  ...  [Diabetes, Insipidus, in, Mice, with, a, Mutat...\n",
              "1       3  ...     [Mice, with, a, Mutation, in, Aquaporin, -, 2]\n",
              "2       5  ...                 [a, Mutation, in, Aquaporin, -, 2]\n",
              "3       8  ...                                  [Aquaporin, -, 2]\n",
              "4      12  ...     [Congenital, nephrogenic, diabetes, insipidus]\n",
              "..    ...  ...                                                ...\n",
              "99    616  ...                                             [AQP2]\n",
              "100   629  ...                                            [water]\n",
              "101   636  ...                                               [it]\n",
              "102   640  ...            [collecting, -, duct, principal, cells]\n",
              "103   646  ...                                      [the, kidney]\n",
              "\n",
              "[104 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7KlBOuZHHpn",
        "outputId": "e7404282-3a95-40c8-d68d-354940dd92b2"
      },
      "source": [
        "#Preparing the gold matrix, this is were we group all te mentions together according to the original file.\n",
        "final_df['temp']=final_df['mention_no']\n",
        "n=len(final_df.index)\n",
        "gold_matrix=np.zeros(shape=(n,n))\n",
        "for i in range(n):\n",
        "  for j in range (n):\n",
        "    if (final_df.iloc[[i]]['mention_no'][i] == final_df.iloc[[j]]['temp'][j]):\n",
        "      gold_matrix[i][j]+=1\n",
        "gold_matrix"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 1., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 1., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 1., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 1., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDne712i3fA-",
        "outputId": "0e59574b-d994-49ed-b958-59ca05d422ce"
      },
      "source": [
        "stacked = np.row_stack(final_df['mean_normalized_vector'])\n",
        "all_pairs_cosine_sim = np.dot(stacked,stacked.T)\n",
        "print(all_pairs_cosine_sim)\n",
        "all_pairs_cosine_sim[all_pairs_cosine_sim < 1.0] = 0\n",
        "all_pairs_cosine_sim[all_pairs_cosine_sim > 1.0] = 1\n",
        "all_pairs_cosine_sim"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.         0.99734218 0.99458091 ... 0.9309727  0.95859348 0.95035466]\n",
            " [0.99734218 1.         0.99742103 ... 0.93294872 0.95938933 0.94754529]\n",
            " [0.99458091 0.99742103 1.         ... 0.93342533 0.95999023 0.94808994]\n",
            " ...\n",
            " [0.9309727  0.93294872 0.93342533 ... 1.         0.90850927 0.92079058]\n",
            " [0.95859348 0.95938933 0.95999023 ... 0.90850927 1.         0.94874468]\n",
            " [0.95035466 0.94754529 0.94808994 ... 0.92079058 0.94874468 1.        ]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 1., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 1., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "faDhlVYRyJg9",
        "outputId": "75859e0e-7586-4f58-ef66-d0290f35d9c0"
      },
      "source": [
        "A=all_pairs_cosine_sim.tolist()\n",
        "A= pd.DataFrame.from_records(A)\n",
        "A=A.to_numpy().flatten()\n",
        "B=gold_matrix.tolist()\n",
        "B = pd.DataFrame.from_records(B)\n",
        "B=B.to_numpy().flatten()\n",
        "print('Confusion matrix is :\\n',confusion_matrix(B,A))\n",
        "print('Accuracy is :',accuracy_score(B,A)*100,'%')\n",
        "print('F1 score is :', f1_score(B,A)*100,'%')"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion matrix is :\n",
            " [[10298    20]\n",
            " [  369   129]]\n",
            "Accuracy is : 96.40347633136095 %\n",
            "F1 score is : 39.876352395672335 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qp_FqzUQWJd0"
      },
      "source": [
        "#Gold standard actual graph. Preparing the graph for the gold standard matrix we have obtained in previous step.\n",
        "#G = nx.DiGraph()\n",
        "#for i in range (len(final_df.index)):\n",
        "  #G.add_node(i,label=final_df['tokens'][i])\n",
        "\n",
        "#for i in range (len(final_df.index)):\n",
        " # for j in range(i):\n",
        "  #  if (final_df['mention_no'][i] == final_df['mention_no'][j]):\n",
        "   #   G.add_edge(i,j,arrowsize=0.0)\n",
        "\n",
        "# set defaults\n",
        "#G.graph['graph']={'rankdir':'TD'}\n",
        "#G.graph['node']={'shape':'rectangle'}\n",
        "#G.graph['edges']={'arrowsize':'0.0'}\n",
        "\n",
        "#A = to_agraph(G)\n",
        "#print(A)\n",
        "#A.layout('circo')\n",
        "#A.draw('gold_standard.png')\n",
        "#img = mpimg.imread('/content/gold_standard.png')\n",
        "#imgplot = plt.imshow(img)\n",
        "#plt.show()"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEYoNb6X7YWS"
      },
      "source": [
        "#Predicted graph\n",
        "#G = nx.DiGraph()\n",
        "#for i in range (len(all_pairs_cosine_sim)):\n",
        "  #G.add_node(i,label=final_df['tokens'][i])\n",
        "  #for j in range (i):\n",
        "    #if (i!=j) and (all_pairs_cosine_sim[i][j]>0.0):\n",
        "      #print('hello')\n",
        "     # G.add_edge(i,j,arrowsize=0.0)\n",
        "\n",
        "# set defaults\n",
        "#G.graph['graph']={'rankdir':'TD'}\n",
        "#G.graph['node']={'shape':'rectangle'}\n",
        "#G.graph['edges']={'arrowsize':'0.0'}\n",
        "\n",
        "#A = to_agraph(G)\n",
        "#print(A)\n",
        "#A.layout('circo')\n",
        "#A.draw('predicted_coreference.png')\n",
        "#img = mpimg.imread('/content/predicted_coreference.png')\n",
        "#imgplot = plt.imshow(img)\n",
        "#plt.show()"
      ],
      "execution_count": 54,
      "outputs": []
    }
  ]
}